# -*- coding: utf-8 -*-
"""Rahul_Employee_Attrition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dqRLm3AoEt4lu7DhqW6xGpkmdZC3g0Xt

## EMPLOYEE ATTRITION ANALYSIS
#### By Rahul Saraf <rahulsaraf62@gmail.com>
#### https://www.kaggle.com/rahulsaraf | https://www.linkedin.com/in/rahulsaraf62 | https://github.com/rahulsaraf62

***Here I have downloaded a dataset "Employee Arrition" from Kaggle.com
https://www.kaggle.com/patelprashant/employee-attrition
It contains employee data and whether or not the employee has left the company he's working in.
Some of the predictor variables include Employee Age, Gender, Education, Monthly Salary, Designation, Distance from home, etc.
The target variable is the Attrtion value, which is True/False.***

Github link for this code is
https://github.com/rahulsaraf62/Employee-Attrition-Analysis.git

### Some variables used in the dataset and its values - 

**Education**

1 'Below College' 

2 'College' 

3 'Bachelor' 

4 'Master' 

5 'Doctor'

**EnvironmentSatisfaction**

1 'Low' 

2 'Medium' 

3 'High' 

4 'Very High


**Job Involvement**


1 'Low' 

2 'Medium' 

3'High' 

4 'Very High'

**Job Satisfaction**

1 'Low' 

2 'Medium' 

3 'High' 

4 'Very High'

**Performance Rating**

1 'Low' 

2 'Good' 

3 'Excellent' 

4 'Outstanding

**Relationship Satisfaction**

1 'Low' 

2 'Medium' 

3 'High' 

4 'Very High

**WorkLifeBalance**

1 'Bad' 

2 'Good' 

3'Better' 

4 'Best

#### Importing the required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""#### Read data into a df called "emp""""

# Importing library to upload files
from google.colab import files

# Installing kaggle library
!pip install -q kaggle

# Uploading the json
upload = files.upload()

# Make a directory named 'kaggle' in root folder. This will let you know if 
# any directory is already present
!mkdir ~/.kaggle

# Copy the jason file there. This should also give you an user access warning
!cp kaggle.json ~/.kaggle/

# Change the permission of the file
!chmod 600 ~/.kaggle/kaggle.json

# Download the data through kaggle API
!kaggle datasets download -d patelprashant/employee-attrition

# Check the files in the directory. You should see the data in zip format
!ls

# Unzip the data
!unzip employee-attrition.zip

emp = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")

"""#### Getting some info about our data"""

emp.info()

"""#### Description of each column """

emp.describe()

"""#### Checking the top 5 rows"""

emp.head(5)

"""## EDA or Exploratory Data Analysis
##### We will do EDA using Python's Seaborn & Matplotlib libraries

### Age of the employees
##### We can see that most of the employees in this data are between 30-40 years of age. The age can be said to be normally distributed.
"""

plt.figure(figsize=(10,7))
sns.countplot(emp['Age'])

"""### Job Roles by Monthly Income
#####  We see that Manager is the highest paid job Role. The Representatives get the least Monthly Income.

"""

sns.barplot(x=emp['MonthlyIncome'], y = emp['JobRole'])

"""### Monthly Income of the employees
#####  We can see that the monthly incomes of most of the employees in this data are between 25,000 to 50,000.
"""

plt.figure(figsize=(15,8))
sns.swarmplot(emp['MonthlyIncome'])

"""### Monthly Income of the employees vs Attrition
####  From the bar plot in the figure, we infer that the employees who have left the company had less average income as compared to the ones who are still in the company.

"""

sns.barplot(x=emp['Attrition'], y = emp['MonthlyIncome'])

"""### Distance from home vs Attrition
####  We can conclude that the employees who have home nearby from their workplace are less likely to leave the company, as we can see from the bottom part of the  violin plot is bulging outwards.
"""

sns.violinplot(x=emp['Attrition'], y = emp['DistanceFromHome'])

"""### Attrition of the employees
####   We can see that for the variable attrition, the frequency of the “No” are much higher than the frequency of “Yes”.
##### Count of employees who have left the company - 494
##### Count of employees working - 2466

"""

sns.countplot(emp['Attrition'])

"""### Gender wise Attrition
####  We conclude that there are more number of male attritions than the number of female attritions in that Company. 
##### Attrition Rate is slightly more in Males as compared to Females.
##### There are more number of male employees as compared to the female employees.
"""

sns.countplot(emp['Gender'],hue=emp['Attrition'])

"""### Department wise Attrition
####  There are three departments in this data, Sales,  R&D, HR.
##### Attrition is highest in R&D dept, then Sales, and least is in the HR department, numerically.
##### As compared, there is least percentage of attrition in the HR department.
"""

sns.countplot(emp['Department'],hue=emp['Attrition'])

"""### Employee Age vs Attrition
##### From the box plot in the figure, we infer that the younger employees within 25-35 years have a higher attrition rate. 
##### More the age less the attrition
##### Attrition rate reduces with age 
"""

sns.boxplot(emp['Attrition'],emp['Age'],width=0.4)

"""### Age vs Department vs Attrition
#####  We can infer from the plot that for HR department, the mean age of employee who will leave the company is the least, and also, the mean age of the employees who will stay in the company is the highest in this department.
##### So, for HR Department, the employees leave the company at a younger age as compared to other departments.
##### We also see that for the Sales department, it is not likely that the employee will leave the company, rather these employees serve the company for more years as compared to other departments.

"""

plt.figure(figsize=(10,6))
sns.boxplot(emp['Department'],emp['Age'],hue=emp['Attrition'], width=0.7)
plt.legend(loc=0)

"""### Age vs Gender vs Attrition
#####  We see that younger females tend to leave the company at a younger age as compared to the male employees. 
"""

plt.figure(figsize=(10,6))
sns.boxplot(x=emp['Attrition'],y=emp['Age'],data=emp, hue=emp['Gender'],width=0.7)
plt.legend(loc=0)

"""### Job Satisfaction vs Attrition
#####  We can deduce that those employees who have Job Satisfaction as “Very High” seem to retain their job, and not leave the company.
##### Whereas, the employees who have low or medium job satisfaction, tend to leave the company, as the percentage of attrition is more in such employees.

"""

fig = plt.figure(figsize=(7,5))
sns.countplot(emp['JobSatisfaction'],hue=emp['Attrition'])
positions = (0, 1, 2, 3)
labels = ('Low','Medium','High','Very High')
plt.xticks(positions, labels)

"""### Monthly Income vs Age of the employees
#####   We can deduce that those employees who are younger, age in 20s and 30s have less monthly income as compared to the elder employees.
#### This is pretty logical also, as with time, employees gain experience, and get promotion and salary hike.

"""

plt.figure(figsize=(15,10))
sns.jointplot(emp['Age'],emp['MonthlyIncome'])

"""## FEATURE ENGINEERING

#### Function to replace Attrition=YES/NO values to 1 and 0
"""

def change_attrition_value(attrition_value):
    if attrition_value=='Yes':
        return 1
    else:
        return 0

emp['Attrition_value']=emp['Attrition'].apply(lambda x : change_attrition_value(x))
emp.drop(['Attrition'],axis=1, inplace=True)

emp['BusinessTravel'].value_counts()

"""#### We will make Dummie Variables for categorical attributes like  
##### Department, Gender, Over Time, Marital Status, Job Role, Education Field 

"""

BusinessTravelDummies = pd.get_dummies(emp['BusinessTravel'], drop_first=True)
emp=emp.join(BusinessTravelDummies)
emp.drop(['BusinessTravel'], axis=1, inplace=True)

emp['Department'].value_counts()

DeptDummies = pd.get_dummies(emp['Department'], drop_first=True)
emp=emp.join(DeptDummies)
emp.drop(['Department'], axis=1, inplace=True)

emp['Gender'].value_counts()

GenderDummies = pd.get_dummies(emp['Gender'], drop_first=True)
emp=emp.join(GenderDummies)
emp.drop(['Gender'], axis=1, inplace=True)

emp['Over18'].value_counts()

emp.drop(['Over18'], axis=1, inplace=True)
#We are dropping over 18 col, since it is of no use here

emp['JobRole'].nunique()

JobRoleDummies = pd.get_dummies(emp['JobRole'], drop_first=True)
emp=emp.join(JobRoleDummies)
emp.drop(['JobRole'], axis=1, inplace=True)

emp['OverTime'].value_counts()

#Make OverTime Dummies(YES/NO values)
OTDummies = pd.get_dummies(emp['OverTime'], drop_first=True)
OTDummies.rename(columns={"Yes":"Over_Time"},inplace=True)
emp=emp.join(OTDummies)
emp.drop(['OverTime'], axis=1, inplace=True)

emp['MaritalStatus'].value_counts()

#Make Marital Status Dummies(Single, Maried or Divorced)
MSDummies = pd.get_dummies(emp['MaritalStatus'], drop_first=True)
emp=emp.join(MSDummies)
emp.drop(['MaritalStatus'], axis=1, inplace=True)

emp['EducationField'].value_counts()

#Make Marital Status Dummies(Single, Maried or Divorced)
EducationFieldDummies = pd.get_dummies(emp['EducationField'], drop_first=True)
emp=emp.join(EducationFieldDummies)
emp.drop(['EducationField'], axis=1, inplace=True)

"""### Train Test Split"""

X = emp.drop(['Attrition_value'],axis=1)

#The value we have to predict (TARGET VARIABLE)
y = emp['Attrition_value']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

"""### Applying Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression

lm = LogisticRegression()

model = lm.fit(X_train,y_train)

predictions = lm.predict(X_test)
predictions_ = lm.predict(X_train)

from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,f1_score

print(classification_report(y_test, predictions))

print(confusion_matrix(y_test, predictions))

print("Accuracy for Test data - ",accuracy_score(y_test, predictions))
print("Accuracy for Train data - ",accuracy_score(y_train, predictions_))

"""#### We get an Accuracy of about 83% on test data and 85 on train data using Logistic Regression.

# Applying Random Forest Model
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

rfc.fit(X_train,y_train)

rfc_pred_test = rfc.predict(X_test)
rfc_pred_train = rfc.predict(X_train)

print("Accuracy for Test data - ",accuracy_score(y_test, rfc_pred_test))
print("Accuracy for Train data - ",accuracy_score(y_train, rfc_pred_train))

"""#### We get accuracy of 85% for Test data and 100% for Train data.

#### Our model is OVERFITTING.

### We do HYPERPARAMETER TESTING using RandomSearch CV to find optimal parameters so that the model is not overfitted.
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {"max_depth": [3, 5, 10],
              "n_estimators": [100,200,300],
              "max_features": randint(1, 5),
              "criterion": ['gini','entropy'],
              "min_samples_split": randint(2, 15),
              "min_samples_leaf": randint(1, 5),
              "bootstrap": [True, False]
             }

random_search = RandomizedSearchCV( RandomForestClassifier(), param_distributions= param_dist, 
                                   n_jobs=-1, n_iter= 50)

random_search.fit(X,y)

randomized_predictions_test = random_search.predict(X_test)
randomized_predictions_train = random_search.predict(X_train)

print("Accuracy for Test data - ",accuracy_score(y_test, randomized_predictions_test))
print("\nAccuracy for Training data - ",accuracy_score(y_train, randomized_predictions_train))

"""#### We can say that now our model is not overfitting as the difference between the test and train accuracies are not very much, they are close."""

print("The parameters are - ",random_search.best_params_)

print(f1_score(y_test, randomized_predictions_test))

print(confusion_matrix(y_test, randomized_predictions_test))

print(classification_report(y_test, randomized_predictions_test))

"""# Finding out which features affect the Target variable the most

#### Using correlation matrix in heatmap
"""

corrmat = emp.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(25,25))
#plot heat map
g=sns.heatmap(emp[top_corr_features].corr(),annot=True,cmap="coolwarm")
plt.savefig('heatmap_corr.png')

"""#### Looking at the heatmap, We conclude that Attrition depends heavily on - OverTime, MaritalStatus, BusinessTravel, DistanceFromHome """